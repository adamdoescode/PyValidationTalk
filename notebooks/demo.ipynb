{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# useful to reload modules when they are modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pd.read_csv is NOT all you need: DataFrame validation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's start by reading in some data I prepared earlier.\n",
    "- Note the use of `DataFrame.dtypes` to check our types. A useful tool!\n",
    "  - Take note of `object` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>int between 0 and 1e6</th>\n",
       "      <th>float between -1 and 1</th>\n",
       "      <th>no nulls allowed</th>\n",
       "      <th>iso 8601 compliant date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>row0</td>\n",
       "      <td>934944</td>\n",
       "      <td>-0.751395</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>row1</td>\n",
       "      <td>842425</td>\n",
       "      <td>0.113027</td>\n",
       "      <td>True</td>\n",
       "      <td>2021-01-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text  int between 0 and 1e6  float between -1 and 1  no nulls allowed  \\\n",
       "0  row0                 934944               -0.751395             False   \n",
       "1  row1                 842425                0.113027              True   \n",
       "\n",
       "  iso 8601 compliant date  \n",
       "0              2021-01-01  \n",
       "1              2021-01-02  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>int between 0 and 1e6</th>\n",
       "      <th>float between -1 and 1</th>\n",
       "      <th>no nulls allowed</th>\n",
       "      <th>iso 8601 compliant date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>row0</td>\n",
       "      <td>934944.0</td>\n",
       "      <td>-0.751395</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>row1</td>\n",
       "      <td>842425.0</td>\n",
       "      <td>0.113027</td>\n",
       "      <td>True</td>\n",
       "      <td>2021-01-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text  int between 0 and 1e6  float between -1 and 1 no nulls allowed  \\\n",
       "0  row0               934944.0               -0.751395            False   \n",
       "1  row1               842425.0                0.113027             True   \n",
       "\n",
       "  iso 8601 compliant date  \n",
       "0              2021-01-01  \n",
       "1              2021-01-02  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "text                        object\n",
       "int between 0 and 1e6        int64\n",
       "float between -1 and 1     float64\n",
       "no nulls allowed              bool\n",
       "iso 8601 compliant date     object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "text                        object\n",
       "int between 0 and 1e6      float64\n",
       "float between -1 and 1     float64\n",
       "no nulls allowed            object\n",
       "iso 8601 compliant date     object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import some good data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# this is the internal module, we use it to get the path to the data\n",
    "from PyVal import config\n",
    "\n",
    "# import the data\n",
    "good_data = pd.read_csv(config.good_data, index_col=0)\n",
    "spicy_data = pd.read_csv(config.spicy_data, index_col=0)\n",
    "# let's see the data\n",
    "display(good_data.head(2), spicy_data.head(2))\n",
    "# and check our dtypes, the first useful data validation trick!\n",
    "display(good_data.dtypes, spicy_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Urgh! Our boolean col is an \"object\" lets add a check to fix it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def fix_no_nulls_cols(column: pd.Series, fill_value=False) -> pd.Series:\n",
    "    \"\"\"Removes nulls from a column\"\"\"\n",
    "    return column.fillna(fill_value).astype(bool)\n",
    "\n",
    "\n",
    "validated = spicy_data.copy()\n",
    "validated[\"no nulls allowed\"] = validated[\"no nulls allowed\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how we can get a numeric `Series` cast to `Object`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64 \t [0. 0.]\n",
      "object \t\t [0.0 0.0 'a']\n"
     ]
    }
   ],
   "source": [
    "# make an array of zeros\n",
    "zeros = pd.Series(np.zeros(2))\n",
    "print(zeros.dtype, \"\\t\", zeros.values)\n",
    "# add a string... and we get an object dtype\n",
    "zeros.loc[2] = \"a\"\n",
    "print(zeros.dtype, \"\\t\\t\", zeros.values)\n",
    "# we need to use `pandas` here because numpy sensibly doesn't let you do this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right... And \"int between 0 and 1e6\" is not an `int` type... because it has a `float` in it 🙃 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     934944.0\n",
       "1     842425.0\n",
       "2     220216.0\n",
       "3     138980.0\n",
       "4     907058.0\n",
       "5     396576.0\n",
       "6     870306.0\n",
       "7     981140.0\n",
       "8     266363.0\n",
       "9     143593.0\n",
       "10        -1.0\n",
       "Name: int between 0 and 1e6, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spicy_data[\"int between 0 and 1e6\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WAIT NO - it has a negative number in it!\n",
    "\n",
    "Checking types didn't even show us that 😰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# and here is a fix... which if you re-run breaks lol 😂\n",
    "validated[\"int between 0 and 1e6\"] = (\n",
    "    validated[\"int between 0 and 1e6\"].astype(int).mask(lambda x: x <= 0, np.nan)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetimes are great and never a problem!!\n",
    "\n",
    "Alright it's fine... at least we can handle the dates right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# all good!\n",
    "validated[\"iso 8601 compliant date\"] = pd.to_datetime(\n",
    "    good_data[\"iso 8601 compliant date\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haha just kidding, we literally can't recover from the classic DD-MM or MM-DD ambiguity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# pd.to_datetime(spicy_data[\"iso 8601 compliant date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S you can usually get around this by knowing some things about your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$ - $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Okay *all* data is a mess, how do we fix it?\n",
    "\n",
    "- manually editing the raw data (bad!!)\n",
    "- adding branching logic whenever you encounter an input data problem (pretty bad)\n",
    "- adding ad-hoc checks in your code (better - but will create a mess)\n",
    "- writing your own validation code that runs before your processing/analysis code (better)\n",
    "- using an existing tool to do the validation for you (ideal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roll your own validation schema\n",
    "\n",
    "Please don't do this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This data is good so we should get to this statement...\n",
      "Task failed succesfully with error: Column 'int between 0 and 1e6' has incorrect data type\n"
     ]
    }
   ],
   "source": [
    "# we need a dodgy function to do the validation with...\n",
    "def validate_data(data: pd.DataFrame, schema: dict) -> pd.DataFrame:\n",
    "    \"\"\"Urgh this is a bit messy to debug if something is broken...\"\"\"\n",
    "    for col, dtype in schema[\"dtypes\"].items():\n",
    "        if col not in data:\n",
    "            raise ValueError(f\"Missing column: {col}\")\n",
    "        if not all(isinstance(val, dtype) for val in data[col]):\n",
    "            raise TypeError(f\"Column '{col}' has incorrect data type\")\n",
    "\n",
    "        checks = schema[\"checks\"].get(col, [])\n",
    "        if checks:\n",
    "            for check in checks:\n",
    "                if not all(check(val) for val in data[col]):\n",
    "                    raise ValueError(f\"Column '{col}' failed validation checks\")\n",
    "    return data\n",
    "\n",
    "\n",
    "# homebrew schema written as nested dict[dict]\n",
    "my_dodgy_schema = {\n",
    "    \"dtypes\": {\n",
    "        \"text\": object,\n",
    "        \"int between 0 and 1e6\": int,\n",
    "        \"float between -1 and 1\": float,\n",
    "        \"no nulls allowed\": bool,\n",
    "    },\n",
    "    \"checks\": {\n",
    "        \"text\": None,\n",
    "        \"int between 0 and 1e6\": [lambda x: 0 <= x <= 1e6],\n",
    "        \"float between -1 and 1\": [lambda x: -1 <= x <= 1],\n",
    "        \"no nulls allowed\": [lambda x: type(x) is bool],\n",
    "    },\n",
    "}\n",
    "\n",
    "# check with known good\n",
    "validated_good = good_data.copy()\n",
    "# Validate good_data\n",
    "try:\n",
    "    validated_good = validate_data(good_data.copy(), my_dodgy_schema)\n",
    "    print(\"This data is good so we should get to this statement...\")\n",
    "except (ValueError, TypeError) as e:\n",
    "    print(f\"Task failed succesfully with error: {e}\")\n",
    "\n",
    "# (Un?)Validate the bad data\n",
    "try:\n",
    "    validated_spicy = validate_data(spicy_data.copy(), my_dodgy_schema)\n",
    "    print(\"Should NOT get to this statement!!\")\n",
    "except (ValueError, TypeError) as e:\n",
    "    print(f\"Task failed succesfully with error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay so that works - why is this bad?\n",
    "\n",
    "- You, the overworked developer who dreams of playing video games tonight, have to maintain it.\n",
    "- You have to test it to make sure it works.\n",
    "- There are a lot of ways validation can go wrong and you have just discovered 0.0001% of those ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seeing the light: Strategies and Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fear not young data tradie! We can stand on the shoulders of giants ❤️\n",
    "- Other people have written tools to help us:\n",
    "  - Pandera validation library\n",
    "  - pola.rs lazy tables\n",
    "  - `pd.DataFrame.astype()` and `pd.to_datetime()`.\n",
    "  - Pydantic\n",
    "  - python *schema*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandera: you could probably get pretty far using this one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you...\n",
    "- are using pandas, pola.rs, dask, etc.\n",
    "- like it when someone else sorted out a lot of the edge-cases for you\n",
    "- deal with small, heterogenous datasets\n",
    "- like clean and simple syntax\n",
    "\n",
    "An example using the schema above:  \n",
    "*Hey Gemini, re-write this custom schema as a Pandera DataFrameSchema:*  \n",
    "*P.S I re-wrote some of this to show off the default check types*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The good dataset passed 😎\n",
      "The bad dataset failed with error: Column 'text' failed element-wise validator number 0: <Check <lambda>> failure cases: 0\n"
     ]
    }
   ],
   "source": [
    "import pandera as pa\n",
    "from pandera.errors import SchemaError  # we use this to catch Pandera validation errors\n",
    "\n",
    "# pandera comes with built in checkers we can use:\n",
    "int_0_to_1e6 = pa.Check.in_range(0, 1e6)\n",
    "float_minus_pos_1 = pa.Check.in_range(-1, 1)\n",
    "# but we can always write our own with lambda or functions.\n",
    "starts_with_row = pa.Check(lambda s: s.str.startswith(\"row\"))\n",
    "\n",
    "# this is how we define a pandera schema\n",
    "my_pandera_schema = pa.DataFrameSchema(\n",
    "    {\n",
    "        \"text\": pa.Column(str, starts_with_row),\n",
    "        \"int between 0 and 1e6\": pa.Column(int, checks=int_0_to_1e6),\n",
    "        \"float between -1 and 1\": pa.Column(float, checks=float_minus_pos_1),\n",
    "        \"no nulls allowed\": pa.Column(bool),\n",
    "    }\n",
    ")\n",
    "\n",
    "good_pandera = my_pandera_schema(good_data)\n",
    "print(\"The good dataset passed 😎\")\n",
    "try:\n",
    "    spicy_pandera = my_pandera_schema(spicy_data)\n",
    "    print(\"If we got here my talk is going poorly 🫢\")\n",
    "except SchemaError as e:\n",
    "    print(f\"The bad dataset failed with error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy Pola.rs 🐻‍❄️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Polars does what Pandas does... but has the benefit of learning from the mistakes Pandas had to discover.\n",
    "- Polars implements `lazy evaluation` through its Lazy API.\n",
    "- This comes with some schema functionality!\n",
    "- But it only covers typing and not validation 🥲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from polars.exceptions import ComputeError\n",
    "\n",
    "\n",
    "def pl_validator(file_path: str) -> pl.DataFrame:\n",
    "    \"\"\"lazy evaluation of a polars DataFrame with a defined schema\"\"\"\n",
    "    schema = {\n",
    "        \"index\": pl.Int64,\n",
    "        \"text\": pl.Utf8,\n",
    "        \"int between 0 and 1e6\": pl.Int32,\n",
    "        \"float between -1 and 1\": pl.Float32,\n",
    "        \"no nulls allowed\": pl.Boolean,\n",
    "        \"iso 8601 compliant date\": pl.Date,\n",
    "    }\n",
    "\n",
    "    return pl.scan_csv(file_path, schema=schema).collect()\n",
    "\n",
    "\n",
    "# Usage with some nice data...\n",
    "good_pl = pl_validator(config.good_data)\n",
    "# and with the spicy data 🌶️\n",
    "try:\n",
    "    spicy_pl = pl_validator(config.spicy_data)\n",
    "    print(\"Oh god, we shouldn't get here... 😱\")\n",
    "except ComputeError as e:\n",
    "    print(f\"Task failed succesfully with error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can always roll our own data-checks, with the caveat its not as easy to follow as `Pandera` or as descriptive.\n",
    "- If anyone knows a neat way to do these checks in Pola.rs feel free to interrupt me! 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task failed succesfully with error: Validation failed, missing rows: 1\n"
     ]
    }
   ],
   "source": [
    "def pl_checker(file_path: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Roll our own checks...\n",
    "    Since filter drops anything that doesn't match the condition we check against\n",
    "    initial row count and fail if the count is less than the original\n",
    "    \"\"\"\n",
    "    # add our checks...\n",
    "    df = pl.scan_csv(file_path)\n",
    "    valid_df = df.filter(\n",
    "        (pl.col(\"int between 0 and 1e6\").is_between(0, 1e6))\n",
    "        & (pl.col(\"float between -1 and 1\").is_between(-1, 1))\n",
    "        & (pl.col(\"text\").str.starts_with(\"row\"))\n",
    "        & (pl.col(\"no nulls allowed\").is_not_null())\n",
    "    ).collect()\n",
    "    df = df.collect()\n",
    "    if len(valid_df) < len(df):\n",
    "        # this is SO BAD. My ignorance of polars is showing here...\n",
    "        # surely there would be a neat way we can do this with polars and have it return descriptive errors\n",
    "        # but then again, maybe you should be using Pandera or some database tooling to do this hey?\n",
    "        raise ValueError(f\"Validation failed, missing rows: {len(df) - len(valid_df)}\")\n",
    "\n",
    "\n",
    "good_pl = pl_checker(config.good_data)\n",
    "try:\n",
    "    spicy_pl = pl_checker(config.spicy_data)\n",
    "    print(\"Please dont print this 😅\")\n",
    "except ValueError as e:\n",
    "    print(f\"Task failed succesfully with error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While pandas doesn't have much validation out of the box it has some *very useful* functions:\n",
    "- `pd.to_datetime`\n",
    "- `pd.Series.astype`\n",
    "- `pd.Series.fillna`\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyValidationTalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
