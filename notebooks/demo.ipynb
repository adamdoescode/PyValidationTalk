{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# useful to reload modules when they are modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pd.read_csv is NOT all you need: DataFrame validation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's start by reading in some data I prepared earlier.\n",
    "- Note the use of `DataFrame.dtypes` to check our types. A useful tool!\n",
    "  - Take note of `object` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_407514/431922942.py:9: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  good_data = pd.read_csv(config.good_data, index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_407514/431922942.py:10: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  spicy_data = pd.read_csv(config.spicy_data, index_col=0, parse_dates=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>int between 0 and 1e6</th>\n",
       "      <th>float between -1 and 1</th>\n",
       "      <th>no nulls allowed</th>\n",
       "      <th>iso 8601 compliant date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>row0</td>\n",
       "      <td>934944</td>\n",
       "      <td>-0.751395</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>row1</td>\n",
       "      <td>842425</td>\n",
       "      <td>0.113027</td>\n",
       "      <td>True</td>\n",
       "      <td>2021-01-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text  int between 0 and 1e6  float between -1 and 1  no nulls allowed  \\\n",
       "0  row0                 934944               -0.751395             False   \n",
       "1  row1                 842425                0.113027              True   \n",
       "\n",
       "  iso 8601 compliant date  \n",
       "0              2021-01-01  \n",
       "1              2021-01-02  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>int between 0 and 1e6</th>\n",
       "      <th>float between -1 and 1</th>\n",
       "      <th>no nulls allowed</th>\n",
       "      <th>iso 8601 compliant date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>row0</td>\n",
       "      <td>934944.0</td>\n",
       "      <td>-0.751395</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>row1</td>\n",
       "      <td>842425.0</td>\n",
       "      <td>0.113027</td>\n",
       "      <td>True</td>\n",
       "      <td>2021-01-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text  int between 0 and 1e6  float between -1 and 1 no nulls allowed  \\\n",
       "0  row0               934944.0               -0.751395            False   \n",
       "1  row1               842425.0                0.113027             True   \n",
       "\n",
       "  iso 8601 compliant date  \n",
       "0              2021-01-01  \n",
       "1              2021-01-02  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "text                        object\n",
       "int between 0 and 1e6        int64\n",
       "float between -1 and 1     float64\n",
       "no nulls allowed              bool\n",
       "iso 8601 compliant date     object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "text                        object\n",
       "int between 0 and 1e6      float64\n",
       "float between -1 and 1     float64\n",
       "no nulls allowed            object\n",
       "iso 8601 compliant date     object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import some good data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# this is the internal module, we use it to get the path to the data\n",
    "from PyVal import config\n",
    "\n",
    "# import the data\n",
    "good_data = pd.read_csv(config.good_data, index_col=0)\n",
    "spicy_data = pd.read_csv(config.spicy_data, index_col=0)\n",
    "# let's see the data\n",
    "display(good_data.head(2), spicy_data.head(2))\n",
    "# and check our dtypes, the first useful data validation trick!\n",
    "display(good_data.dtypes, spicy_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Urgh! Our boolean col is an \"object\" lets add a check to fix it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def fix_no_nulls_cols(column: pd.Series, fill_value=False) -> pd.Series:\n",
    "    \"\"\"Removes nulls from a column\"\"\"\n",
    "    return column.fillna(fill_value).astype(bool)\n",
    "\n",
    "\n",
    "validated = spicy_data.copy()\n",
    "validated[\"no nulls allowed\"] = validated[\"no nulls allowed\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how we can get a numeric `Series` cast to `Object`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64 \t [0. 0.]\n",
      "object \t\t [0.0 0.0 'a']\n"
     ]
    }
   ],
   "source": [
    "# make an array of zeros\n",
    "zeros = pd.Series(np.zeros(2))\n",
    "print(zeros.dtype, \"\\t\", zeros.values)\n",
    "# add a string... and we get an object dtype\n",
    "zeros.loc[2] = \"a\"\n",
    "print(zeros.dtype, \"\\t\\t\", zeros.values)\n",
    "# we need to use `pandas` here because numpy sensibly doesn't let you do this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right... And \"int between 0 and 1e6\" is not an `int` type... because it has a `float` in it 🙃 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     934944.0\n",
       "1     842425.0\n",
       "2     220216.0\n",
       "3     138980.0\n",
       "4     907058.0\n",
       "5     396576.0\n",
       "6     870306.0\n",
       "7     981140.0\n",
       "8     266363.0\n",
       "9     143593.0\n",
       "10        -1.0\n",
       "Name: int between 0 and 1e6, dtype: float64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spicy_data[\"int between 0 and 1e6\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WAIT NO - it has a negative number in it!\n",
    "\n",
    "Checking types didn't even show us that 😰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# and here is a fix... which if you re-run breaks lol 😂\n",
    "validated[\"int between 0 and 1e6\"] = (\n",
    "    validated[\"int between 0 and 1e6\"].astype(int).mask(lambda x: x <= 0, np.nan)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetimes are great and never a problem!!\n",
    "\n",
    "Alright it's fine... at least we can handle the dates right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# all good!\n",
    "validated[\"iso 8601 compliant date\"] = pd.to_datetime(\n",
    "    good_data[\"iso 8601 compliant date\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haha just kidding, we literally can't recover from the classic DD-MM or MM-DD ambiguity:\n",
    "- On the bright side pandas is doing validation for us here.\n",
    "- It is, righly, breaking here because we have a critical error we need to deal with.\n",
    "- You can usually get around these ambiguities by knowing some things about your dataset and fixing accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data \"10-10-2021\" doesn't match format \"%Y-%m-%d\", at position 10. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspicy_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miso 8601 compliant date\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/PyValidationTalk/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:1067\u001b[0m, in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1065\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1067\u001b[0m         values \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1068\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39m_constructor(values, index\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc\u001b[38;5;241m.\u001b[39mMutableMapping)):\n",
      "File \u001b[0;32m~/miniconda3/envs/PyValidationTalk/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:433\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_strptime_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m objects_to_datetime64(\n\u001b[1;32m    436\u001b[0m     arg,\n\u001b[1;32m    437\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    441\u001b[0m     allow_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    442\u001b[0m )\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/PyValidationTalk/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:467\u001b[0m, in \u001b[0;36m_array_strptime_with_fallback\u001b[0;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_array_strptime_with_fallback\u001b[39m(\n\u001b[1;32m    457\u001b[0m     arg,\n\u001b[1;32m    458\u001b[0m     name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    462\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    463\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[1;32m    464\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;124;03m    Call array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m     result, tz_out \u001b[38;5;241m=\u001b[39m \u001b[43marray_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tz_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m         unit \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdatetime_data(result\u001b[38;5;241m.\u001b[39mdtype)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32mstrptime.pyx:501\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mstrptime.pyx:451\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mstrptime.pyx:583\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime._parse_with_format\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: time data \"10-10-2021\" doesn't match format \"%Y-%m-%d\", at position 10. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "pd.to_datetime(spicy_data[\"iso 8601 compliant date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8     2021-01-09\n",
       "9     2021-01-10\n",
       "10    10-10-2021\n",
       "Name: iso 8601 compliant date, dtype: object"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spicy_data[\"iso 8601 compliant date\"].tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$ - $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Okay *all* data is a mess, how do we fix it?\n",
    "\n",
    "- manually editing the raw data (bad!!)\n",
    "- adding branching logic whenever you encounter an input data problem (pretty bad)\n",
    "- adding ad-hoc checks in your code (better - but will create a mess)\n",
    "- writing your own validation code that runs before your processing/analysis code (better)\n",
    "- using an existing tool to do the validation for you (ideal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roll your own validation schema\n",
    "\n",
    "Please don't do this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This data is good so we should get to this statement...\n",
      "Task failed succesfully with error: Column 'int between 0 and 1e6' has incorrect data type\n"
     ]
    }
   ],
   "source": [
    "# we need a dodgy function to do the validation with...\n",
    "def validate_data(data: pd.DataFrame, schema: dict) -> pd.DataFrame:\n",
    "    \"\"\"Urgh this is a bit messy to debug if something is broken...\"\"\"\n",
    "    for col, dtype in schema[\"dtypes\"].items():\n",
    "        if col not in data:\n",
    "            raise ValueError(f\"Missing column: {col}\")\n",
    "        if not all(isinstance(val, dtype) for val in data[col]):\n",
    "            raise TypeError(f\"Column '{col}' has incorrect data type\")\n",
    "\n",
    "        checks = schema[\"checks\"].get(col, [])\n",
    "        if checks:\n",
    "            for check in checks:\n",
    "                if not all(check(val) for val in data[col]):\n",
    "                    raise ValueError(f\"Column '{col}' failed validation checks\")\n",
    "    return data\n",
    "\n",
    "\n",
    "# homebrew schema written as nested dict[dict]\n",
    "my_dodgy_schema = {\n",
    "    \"dtypes\": {\n",
    "        \"text\": object,\n",
    "        \"int between 0 and 1e6\": int,\n",
    "        \"float between -1 and 1\": float,\n",
    "        \"no nulls allowed\": bool,\n",
    "        \"iso 8601 compliant date\": object,\n",
    "    },\n",
    "    \"checks\": {\n",
    "        \"text\": None,\n",
    "        \"int between 0 and 1e6\": [lambda x: 0 <= x <= 1e6],\n",
    "        \"float between -1 and 1\": [lambda x: -1 <= x <= 1],\n",
    "        \"no nulls allowed\": [lambda x: type(x) is bool],\n",
    "        \"iso 8601 compliant date\": [lambda x: pd.to_datetime(x, errors=\"coerce\") is not None],\n",
    "    },\n",
    "}\n",
    "\n",
    "# check with known good\n",
    "validated_good = good_data.copy()\n",
    "# Validate good_data\n",
    "try:\n",
    "    validated_good = validate_data(good_data.copy(), my_dodgy_schema)\n",
    "    print(\"This data is good so we should get to this statement...\")\n",
    "except (ValueError, TypeError) as e:\n",
    "    print(f\"Task failed succesfully with error: {e}\")\n",
    "\n",
    "# (Un?)Validate the bad data\n",
    "try:\n",
    "    validated_spicy = validate_data(spicy_data.copy(), my_dodgy_schema)\n",
    "    print(\"Should NOT get to this statement!!\")\n",
    "except (ValueError, TypeError) as e:\n",
    "    print(f\"Task failed succesfully with error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay so that works - why is this bad?\n",
    "\n",
    "- You, the overworked developer who dreams of playing video games tonight, have to maintain it.\n",
    "- You have to test it to make sure it works.\n",
    "- There are a lot of ways validation can go wrong and you have just discovered 0.0001% of those ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandera: you could probably get pretty far using this one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you...\n",
    "- are using pandas, pola.rs, dask, etc.\n",
    "- like it when someone else sorted out a lot of the edge-cases for you\n",
    "- deal with small, heterogenous datasets\n",
    "- like clean and simple syntax\n",
    "\n",
    "An example using the schema above:  \n",
    "*Hey Gemini, re-write this custom schema as a Pandera DataFrameSchema:*  \n",
    "*P.S I re-wrote some of this to show off the default check types*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The good dataset passed 😎\n",
      "The bad dataset failed with errors:\n",
      "Column 'text' failed element-wise validator number 0: <Check <lambda>> failure cases: 0\n"
     ]
    }
   ],
   "source": [
    "import pandera as pa\n",
    "from pandera.errors import SchemaError, SchemaErrors  # we use this to catch Pandera validation errors\n",
    "\n",
    "# pandera comes with built in checkers we can use:\n",
    "int_0_to_1e6 = pa.Check.in_range(0, 1e6)\n",
    "float_minus_pos_1 = pa.Check.in_range(-1, 1)\n",
    "# but we can always write our own with lambda or functions.\n",
    "starts_with_row = pa.Check(lambda s: s.str.startswith(\"row\"))\n",
    "coerce_datetime = pa.Check(lambda s: pd.to_datetime(s) is not None)\n",
    "\n",
    "# this is how we define a pandera schema\n",
    "my_pandera_schema = pa.DataFrameSchema(\n",
    "    {\n",
    "        \"text\": pa.Column(str, starts_with_row),\n",
    "        \"int between 0 and 1e6\": pa.Column(int, checks=int_0_to_1e6),\n",
    "        \"float between -1 and 1\": pa.Column(float, checks=float_minus_pos_1),\n",
    "        \"no nulls allowed\": pa.Column(bool),\n",
    "        \"iso 8601 compliant date\": pa.Column(str, checks=coerce_datetime),\n",
    "    }\n",
    ")\n",
    "\n",
    "good_pandera = my_pandera_schema(good_data)\n",
    "print(\"The good dataset passed 😎\")\n",
    "try:\n",
    "    spicy_pandera = my_pandera_schema(spicy_data)\n",
    "    print(\"If we got here my talk is going poorly 🫢\")\n",
    "except SchemaError as e:\n",
    "    print(f\"The bad dataset failed with errors:\\n{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we use `lazy=True` when calling the validator we can see all the errors instead of just the first one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bad dataset failed with errors:\n",
      "{\n",
      "    \"DATA\": {\n",
      "        \"DATAFRAME_CHECK\": [\n",
      "            {\n",
      "                \"schema\": null,\n",
      "                \"column\": \"text\",\n",
      "                \"check\": \"<lambda>\",\n",
      "                \"error\": \"Column 'text' failed element-wise validator number 0: <Check <lambda>> failure cases: 0\"\n",
      "            },\n",
      "            {\n",
      "                \"schema\": null,\n",
      "                \"column\": \"int between 0 and 1e6\",\n",
      "                \"check\": \"in_range(0, 1000000.0)\",\n",
      "                \"error\": \"Column 'int between 0 and 1e6' failed element-wise validator number 0: in_range(0, 1000000.0) failure cases: -1.0\"\n",
      "            },\n",
      "            {\n",
      "                \"schema\": null,\n",
      "                \"column\": \"float between -1 and 1\",\n",
      "                \"check\": \"in_range(-1, 1)\",\n",
      "                \"error\": \"Column 'float between -1 and 1' failed element-wise validator number 0: in_range(-1, 1) failure cases: 6.0\"\n",
      "            }\n",
      "        ],\n",
      "        \"CHECK_ERROR\": [\n",
      "            {\n",
      "                \"schema\": null,\n",
      "                \"column\": \"iso 8601 compliant date\",\n",
      "                \"check\": \"<lambda>\",\n",
      "                \"error\": \"Error while executing check function: ValueError(\\\"time data \\\"10-10-2021\\\" doesn't match format \\\"%Y-%m-%d\\\", at position 10. You might want to try:    - passing `format` if your strings have a consistent format;    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.\\\")Traceback (most recent call last):  File \\\"/home/sp/miniconda3/envs/PyValidationTalk/lib/python3.10/site-packages/pandera/backends/pandas/components.py\\\", line 235, in run_checks    self.run_check(  File \\\"/home/sp/miniconda3/envs/PyValidationTalk/lib/python3.10/site-packages/pandera/backends/pandas/base.py\\\", line 115, in run_check    check_result: CheckResult = check(check_obj, *args)  File \\\"/home/sp/miniconda3/envs/PyValidationTalk/lib/python3.10/site-packages/pandera/api/checks.py\\\", line 234, in __call__    return backend(check_obj, column)  File \\\"/home/sp/miniconda3/envs/PyValidationTalk/lib/python3.10/site-packages/pandera/backends/pandas/checks.py\\\", line 348, in __call__    check_output = self.apply(check_obj)  File \\\"/home/sp/miniconda3/envs/PyValidationTalk/lib/python3.10/site-packages/pandera/backends/pandas/checks.py\\\", line 147, in apply    return apply_fn(check_obj)  File \\\"/home/sp/miniconda3/envs/PyValidationTalk/lib/python3.10/site-packages/pandera/backends/pandas/checks.py\\\", line 155, in apply_field    return self.check_fn(check_obj)  File \\\"/tmp/ipykernel_407514/774460812.py\\\", line 9, in <lambda>    coerce_datetime = pa.Check(lambda s: pd.to_datetime(s) is not None)  File \\\"/home/sp/miniconda3/envs/PyValidationTalk/lib/python3.10/site-packages/pandas/core/tools/datetimes.py\\\", line 1067, in to_datetime    values = convert_listlike(arg._values, format)  File \\\"/home/sp/miniconda3/envs/PyValidationTalk/lib/python3.10/site-packages/pandas/core/tools/datetimes.py\\\", line 433, in _convert_listlike_datetimes    return _array_strptime_with_fallback(arg, name, utc, format, exact, errors)  File \\\"/home/sp/miniconda3/envs/PyValidationTalk/lib/python3.10/site-packages/pandas/core/tools/datetimes.py\\\", line 467, in _array_strptime_with_fallback    result, tz_out = array_strptime(arg, fmt, exact=exact, errors=errors, utc=utc)  File \\\"strptime.pyx\\\", line 501, in pandas._libs.tslibs.strptime.array_strptime  File \\\"strptime.pyx\\\", line 451, in pandas._libs.tslibs.strptime.array_strptime  File \\\"strptime.pyx\\\", line 583, in pandas._libs.tslibs.strptime._parse_with_formatValueError: time data \\\"10-10-2021\\\" doesn't match format \\\"%Y-%m-%d\\\", at position 10. You might want to try:    - passing `format` if your strings have a consistent format;    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"SCHEMA\": {\n",
      "        \"WRONG_DATATYPE\": [\n",
      "            {\n",
      "                \"schema\": null,\n",
      "                \"column\": \"int between 0 and 1e6\",\n",
      "                \"check\": \"dtype('int64')\",\n",
      "                \"error\": \"expected series 'int between 0 and 1e6' to have type int64, got float64\"\n",
      "            },\n",
      "            {\n",
      "                \"schema\": null,\n",
      "                \"column\": \"no nulls allowed\",\n",
      "                \"check\": \"dtype('bool')\",\n",
      "                \"error\": \"expected series 'no nulls allowed' to have type bool, got object\"\n",
      "            }\n",
      "        ],\n",
      "        \"SERIES_CONTAINS_NULLS\": [\n",
      "            {\n",
      "                \"schema\": null,\n",
      "                \"column\": \"no nulls allowed\",\n",
      "                \"check\": \"not_nullable\",\n",
      "                \"error\": \"non-nullable series 'no nulls allowed' contains null values:10    NaNName: no nulls allowed, dtype: object\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sp/miniconda3/envs/PyValidationTalk/lib/python3.10/site-packages/pandera/backends/pandas/error_formatters.py:268: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  concat_fn(check_failure_cases)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spicy_pandera = my_pandera_schema(spicy_data, lazy=True)\n",
    "    print(\"If we got here my talk is going poorly 🫢\")\n",
    "except SchemaErrors as e:\n",
    "    print(f\"The bad dataset failed with errors:\\n{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy Pola.rs 🐻‍❄️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Polars does what Pandas does... but has the benefit of learning from the mistakes Pandas had to discover.\n",
    "- Polars implements `lazy evaluation` through its Lazy API.\n",
    "- This comes with some schema functionality!\n",
    "- But it only covers typing and not validation 🥲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from polars.exceptions import ComputeError\n",
    "\n",
    "\n",
    "def pl_validator(file_path: str) -> pl.DataFrame:\n",
    "    \"\"\"lazy evaluation of a polars DataFrame with a defined schema\"\"\"\n",
    "    schema = {\n",
    "        \"index\": pl.Int64,\n",
    "        \"text\": pl.Utf8,\n",
    "        \"int between 0 and 1e6\": pl.Int32,\n",
    "        \"float between -1 and 1\": pl.Float32,\n",
    "        \"no nulls allowed\": pl.Boolean,\n",
    "        \"iso 8601 compliant date\": pl.Date,\n",
    "    }\n",
    "\n",
    "    return pl.scan_csv(file_path, schema=schema).collect()\n",
    "\n",
    "\n",
    "# Usage with some nice data...\n",
    "good_pl = pl_validator(config.good_data)\n",
    "# and with the spicy data 🌶️\n",
    "try:\n",
    "    spicy_pl = pl_validator(config.spicy_data)\n",
    "    print(\"Oh god, we shouldn't get here... 😱\")\n",
    "except ComputeError as e:\n",
    "    print(f\"Task failed succesfully with error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can always roll our own data-checks, with the caveat its not as easy to follow as `Pandera` or as descriptive.\n",
    "- If anyone knows a neat way to do these checks in Pola.rs feel free to interrupt me! 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task failed succesfully with error: Validation failed, missing rows: 1\n"
     ]
    }
   ],
   "source": [
    "def pl_checker(file_path: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Roll our own checks...\n",
    "    Since filter drops anything that doesn't match the condition we check against\n",
    "    initial row count and fail if the count is less than the original\n",
    "    \"\"\"\n",
    "    # add our checks...\n",
    "    df = pl.scan_csv(file_path)\n",
    "    valid_df = df.filter(\n",
    "        (pl.col(\"int between 0 and 1e6\").is_between(0, 1e6))\n",
    "        & (pl.col(\"float between -1 and 1\").is_between(-1, 1))\n",
    "        & (pl.col(\"text\").str.starts_with(\"row\"))\n",
    "        & (pl.col(\"no nulls allowed\").is_not_null())\n",
    "    ).collect()\n",
    "    df = df.collect()\n",
    "    if len(valid_df) < len(df):\n",
    "        # this is SO BAD. My ignorance of polars is showing here...\n",
    "        # surely there would be a neat way we can do this with polars and have it return descriptive errors\n",
    "        # but then again, maybe you should be using Pandera or some database tooling to do this hey?\n",
    "        raise ValueError(f\"Validation failed, missing rows: {len(df) - len(valid_df)}\")\n",
    "    return valid_df\n",
    "\n",
    "\n",
    "good_pl = pl_checker(config.good_data)\n",
    "try:\n",
    "    spicy_pl = pl_checker(config.spicy_data)\n",
    "    print(\"Please dont print this 😅\")\n",
    "except ValueError as e:\n",
    "    print(f\"Task failed succesfully with error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pydantic\n",
    "\n",
    "- We're going to use version >= v2.10.0\n",
    "- Advantages: ✅\n",
    "  - You may want to use this anyway for validating your *not table* data\n",
    "  - It has a rich feature set for validation\n",
    "- Disadvantages: 🚫\n",
    "  - Not beginner friendly!\n",
    "  - Big API change in v2 which means DeepSeek and Stackoverflow often suggest the wrong thing.\n",
    "  - Not designed for tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: `Field(ge=0, le=1e6)` will check that `0 >= x <= 1e6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The good dataset passed 😎\n",
      "The bad dataset failed with error: 4 validation errors for MyModelValidator\n",
      "text\n",
      "  Value error, must start with \"row\" [type=value_error, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/value_error\n",
      "int_between_0_and_1e6\n",
      "  Input should be greater than or equal to 0 [type=greater_than_equal, input_value=-1.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/greater_than_equal\n",
      "float_between_minus_1_and_1\n",
      "  Input should be less than or equal to 1 [type=less_than_equal, input_value=6.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/less_than_equal\n",
      "no_nulls_allowed\n",
      "  Input should be a valid boolean [type=bool_type, input_value=nan, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/bool_type\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field, field_validator, ValidationError\n",
    "\n",
    "class MyModelValidator(BaseModel):\n",
    "    text: str\n",
    "    int_between_0_and_1e6: int = Field(ge=0, le=1e6)\n",
    "    float_between_minus_1_and_1: float = Field(ge=-1, le=1)\n",
    "    no_nulls_allowed: bool\n",
    "\n",
    "    @field_validator('text')\n",
    "    def text_must_start_with_row(cls, v):\n",
    "        if not v.startswith(\"row\"):\n",
    "            raise ValueError('must start with \"row\"')\n",
    "        return v\n",
    "\n",
    "def munge_colnames_for_pydantic(df:pd.DataFrame) -> list[dict]:\n",
    "    df.columns = [c.replace(\" \", \"_\").replace(\"-\",\"minus_\") for c in df.columns]\n",
    "    return df.to_dict(orient=\"records\")\n",
    "# Validate good data\n",
    "try:\n",
    "    for row in munge_colnames_for_pydantic(good_data):\n",
    "        MyModelValidator(**row)\n",
    "    print(\"The good dataset passed 😎\")\n",
    "except ValidationError as e:\n",
    "    print(f\"Haha oh no; error: {e}\")\n",
    "\n",
    "# Validate spicy data\n",
    "try:\n",
    "    for row in munge_colnames_for_pydantic(spicy_data):\n",
    "        MyModelValidator(**row)\n",
    "    print(\"Everyone will find out I just used chatGPT 😭\")\n",
    "except ValidationError as e:\n",
    "    print(f\"The bad dataset failed with error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Protip Pandera has a DataFrameModel that plays nicely with Pydantic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While pandas doesn't have much validation out of the box it has some *very useful* functions:\n",
    "- `pd.to_datetime`\n",
    "- `pd.Series.astype`\n",
    "- `pd.Series.fillna`\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyValidationTalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
