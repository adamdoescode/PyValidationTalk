{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# useful to reload modules when they are modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pd.read_csv is NOT all you need: DataFrame validation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's start by reading in some data I prepared earlier.\n",
    "- Note the use of `DataFrame.dtypes` to check our types. A useful tool!\n",
    "  - Take note of `object` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>int between 0 and 1e6</th>\n",
       "      <th>float between -1 and 1</th>\n",
       "      <th>no nulls allowed</th>\n",
       "      <th>iso 8601 compliant date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>row0</td>\n",
       "      <td>934944</td>\n",
       "      <td>-0.751395</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>row1</td>\n",
       "      <td>842425</td>\n",
       "      <td>0.113027</td>\n",
       "      <td>True</td>\n",
       "      <td>2021-01-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text  int between 0 and 1e6  float between -1 and 1  no nulls allowed  \\\n",
       "0  row0                 934944               -0.751395             False   \n",
       "1  row1                 842425                0.113027              True   \n",
       "\n",
       "  iso 8601 compliant date  \n",
       "0              2021-01-01  \n",
       "1              2021-01-02  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>int between 0 and 1e6</th>\n",
       "      <th>float between -1 and 1</th>\n",
       "      <th>no nulls allowed</th>\n",
       "      <th>iso 8601 compliant date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>row0</td>\n",
       "      <td>934944.0</td>\n",
       "      <td>-0.751395</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>row1</td>\n",
       "      <td>842425.0</td>\n",
       "      <td>0.113027</td>\n",
       "      <td>True</td>\n",
       "      <td>2021-01-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text  int between 0 and 1e6  float between -1 and 1 no nulls allowed  \\\n",
       "0  row0               934944.0               -0.751395            False   \n",
       "1  row1               842425.0                0.113027             True   \n",
       "\n",
       "  iso 8601 compliant date  \n",
       "0              2021-01-01  \n",
       "1              2021-01-02  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "text                        object\n",
       "int between 0 and 1e6        int64\n",
       "float between -1 and 1     float64\n",
       "no nulls allowed              bool\n",
       "iso 8601 compliant date     object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "text                        object\n",
       "int between 0 and 1e6      float64\n",
       "float between -1 and 1     float64\n",
       "no nulls allowed            object\n",
       "iso 8601 compliant date     object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import some good data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# this is the internal module, we use it to get the path to the data\n",
    "from PyVal import config\n",
    "\n",
    "# import the data\n",
    "good_data = pd.read_csv(config.good_data, index_col=0)\n",
    "spicy_data = pd.read_csv(config.spicy_data, index_col=0)\n",
    "# let's see the data\n",
    "display(good_data.head(2), spicy_data.head(2))\n",
    "# and check our dtypes, the first useful data validation trick!\n",
    "display(good_data.dtypes, spicy_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Urgh! Our boolean col is an `object` and not a `bool` lets add a check to fix it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def fix_no_nulls_cols(column: pd.Series, fill_value=False) -> pd.Series:\n",
    "    \"\"\"Removes nulls from a column\"\"\"\n",
    "    return column.fillna(fill_value).astype(bool)\n",
    "\n",
    "\n",
    "validated = spicy_data.copy()\n",
    "validated[\"no nulls allowed\"] = validated[\"no nulls allowed\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right... And \"int between 0 and 1e6\" is not an `int` type... because it has a `float` in it ðŸ™ƒ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     934944.0\n",
       "1     842425.0\n",
       "2     220216.0\n",
       "3     138980.0\n",
       "4     907058.0\n",
       "5     396576.0\n",
       "6     870306.0\n",
       "7     981140.0\n",
       "8     266363.0\n",
       "9     143593.0\n",
       "10        -1.0\n",
       "Name: int between 0 and 1e6, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spicy_data[\"int between 0 and 1e6\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WAIT NO - it has a negative number in it!\n",
    "\n",
    "Checking types didn't even show us that ðŸ˜°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# and here is a fix... which if you re-run breaks lol ðŸ˜‚\n",
    "validated[\"int between 0 and 1e6\"] = (\n",
    "    validated[\"int between 0 and 1e6\"].astype(int).mask(lambda x: x <= 0, np.nan)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetimes are great and never a problem!!\n",
    "\n",
    "Alright it's fine... at least we can handle the dates right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# all good!\n",
    "validated[\"iso 8601 compliant date\"] = pd.to_datetime(\n",
    "    good_data[\"iso 8601 compliant date\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haha just kidding, we literally can't recover from the classic DD-MM or MM-DD ambiguity:\n",
    "- On the bright side pandas is doing validation for us here.\n",
    "- It is, righly, breaking here because we have a critical error we need to deal with.\n",
    "- You can usually get around these ambiguities by knowing some things about your dataset and fixing accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# pd.to_datetime(spicy_data[\"iso 8601 compliant date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8     2021-01-09\n",
       "9     2021-01-10\n",
       "10    10-10-2021\n",
       "Name: iso 8601 compliant date, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spicy_data[\"iso 8601 compliant date\"].tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$ - $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Okay *all* data is a mess, how do we fix it?\n",
    "\n",
    "- manually editing the raw data (bad!!)\n",
    "- adding branching logic whenever you encounter an input data problem (pretty bad)\n",
    "- adding ad-hoc checks in your code (better - but will create a mess)\n",
    "- writing your own validation code that runs before your processing/analysis code (better)\n",
    "- using an existing tool to do the validation for you (ideal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roll your own validation schema\n",
    "\n",
    "Please don't do this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This data is good so we should get to this statement...\n",
      "Task failed succesfully with error: Column 'int between 0 and 1e6' has incorrect data type\n"
     ]
    }
   ],
   "source": [
    "# we need a dodgy function to do the validation with...\n",
    "def validate_data(data: pd.DataFrame, schema: dict) -> pd.DataFrame:\n",
    "    \"\"\"Urgh this is a bit messy to debug if something is broken...\"\"\"\n",
    "    for col, dtype in schema[\"dtypes\"].items():\n",
    "        if col not in data:\n",
    "            raise ValueError(f\"Missing column: {col}\")\n",
    "        if not all(isinstance(val, dtype) for val in data[col]):\n",
    "            raise TypeError(f\"Column '{col}' has incorrect data type\")\n",
    "\n",
    "        checks = schema[\"checks\"].get(col, [])\n",
    "        if checks:\n",
    "            for check in checks:\n",
    "                if not all(check(val) for val in data[col]):\n",
    "                    raise ValueError(f\"Column '{col}' failed validation checks\")\n",
    "    return data\n",
    "\n",
    "\n",
    "# homebrew schema written as nested dict[dict]\n",
    "my_dodgy_schema = {\n",
    "    \"dtypes\": {\n",
    "        \"text\": object,\n",
    "        \"int between 0 and 1e6\": int,\n",
    "        \"float between -1 and 1\": float,\n",
    "        \"no nulls allowed\": bool,\n",
    "        \"iso 8601 compliant date\": object,\n",
    "    },\n",
    "    \"checks\": {\n",
    "        \"text\": None,\n",
    "        \"int between 0 and 1e6\": [lambda x: 0 <= x <= 1e6],\n",
    "        \"float between -1 and 1\": [lambda x: -1 <= x <= 1],\n",
    "        \"no nulls allowed\": [lambda x: type(x) is bool],\n",
    "        \"iso 8601 compliant date\": [\n",
    "            lambda x: pd.to_datetime(x, errors=\"coerce\") is not None\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "# check with known good\n",
    "validated_good = good_data.copy()\n",
    "# Validate good_data\n",
    "try:\n",
    "    validated_good = validate_data(good_data.copy(), my_dodgy_schema)\n",
    "    print(\"This data is good so we should get to this statement...\")\n",
    "except (ValueError, TypeError) as e:\n",
    "    print(f\"Task failed succesfully with error: {e}\")\n",
    "\n",
    "# (Un?)Validate the bad data\n",
    "try:\n",
    "    validated_spicy = validate_data(spicy_data.copy(), my_dodgy_schema)\n",
    "    print(\"Should NOT get to this statement!!\")\n",
    "except (ValueError, TypeError) as e:\n",
    "    print(f\"Task failed succesfully with error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay so that works - why is this bad?\n",
    "\n",
    "- You, the overworked developer who dreams of playing video games tonight, have to maintain it.\n",
    "- You have to test it to make sure it works.\n",
    "- There are a lot of ways validation can go wrong and you have just discovered 0.0001% of those ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandera: pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you...\n",
    "- are using pandas, pola.rs, dask, etc.\n",
    "- like it when someone else sorted out a lot of the edge-cases for you\n",
    "- deal with small, heterogenous datasets\n",
    "- like clean and simple syntax\n",
    "\n",
    "An example using the schema above:  \n",
    "*Hey Gemini, re-write this custom schema as a Pandera DataFrameSchema:*  \n",
    "*P.S I re-wrote some of this to show off the default check types*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The good dataset passed ðŸ˜Ž\n",
      "The bad dataset failed with errors:\n",
      "Column 'text' failed element-wise validator number 0: <Check <lambda>> failure cases: 0\n"
     ]
    }
   ],
   "source": [
    "import pandera as pa\n",
    "from pandera.errors import (\n",
    "    SchemaError,\n",
    "    SchemaErrors,\n",
    ")  # we use this to catch Pandera validation errors\n",
    "\n",
    "# pandera comes with built in checkers we can use:\n",
    "int_0_to_1e6 = pa.Check.in_range(0, 1e6)\n",
    "float_minus_pos_1 = pa.Check.in_range(-1, 1)\n",
    "# but we can always write our own with lambda or functions.\n",
    "starts_with_row = pa.Check(lambda s: s.str.startswith(\"row\"))\n",
    "coerce_datetime = pa.Check(lambda s: pd.to_datetime(s) is not None)\n",
    "\n",
    "# this is how we define a pandera schema\n",
    "my_pandera_schema = pa.DataFrameSchema(\n",
    "    {\n",
    "        \"text\": pa.Column(str, starts_with_row),\n",
    "        \"int between 0 and 1e6\": pa.Column(int, checks=int_0_to_1e6),\n",
    "        \"float between -1 and 1\": pa.Column(float, checks=float_minus_pos_1),\n",
    "        \"no nulls allowed\": pa.Column(bool),\n",
    "        \"iso 8601 compliant date\": pa.Column(str, checks=coerce_datetime),\n",
    "    }\n",
    ")\n",
    "\n",
    "good_pandera = my_pandera_schema(good_data)\n",
    "print(\"The good dataset passed ðŸ˜Ž\")\n",
    "try:\n",
    "    spicy_pandera = my_pandera_schema(spicy_data)\n",
    "    print(\"If we got here my talk is going poorly ðŸ«¢\")\n",
    "except SchemaError as e:\n",
    "    print(f\"The bad dataset failed with errors:\\n{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we use `lazy=True` when calling the validator we can see all the errors instead of just the first one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bad dataset failed with errors:\n",
      "{\n",
      "    \"DATA\": {\n",
      "        \"DATAFRAME_CHECK\": [\n",
      "            {\n",
      "                \"schema\": null,\n",
      "                \"column\": \"text\",\n",
      "                \"check\": \"<lambda>\",\n",
      "                \"error\": \"Column 'text' failed element-wise validator number 0: <Check <lambda>> failure cases: 0\"\n",
      "            },\n",
      "            {\n",
      "                \"schema\": null,\n",
      "                \"column\": \"int between 0 and 1e6\",\n",
      "                \"check\": \"in_range(0, 1000000.0)\",\n",
      "                \"error\": \"Column 'int between 0 and 1e6' failed element-wise validator number 0: in_range(0, 1000000.0) failure cases: -1.0\"\n",
      "            },\n",
      "            {\n",
      "                \"schema\": null,\n",
      "                \"column\": \"float between -1 and 1\",\n",
      "                \"check\": \"in_range(-1, 1)\",\n",
      "                \"error\": \"Column 'float between -1 and 1' failed element-wise validator number 0: in_range(-1, 1) failure cases: 6.0\"\n",
      "            }\n",
      "        ],\n",
      "        \"CHECK_ERROR\": [\n",
      "            {\n",
      "                \"schema\": null,\n",
      "                \"column\": \"iso 8601 compliant date\",\n",
      "                \"check\": \"<lambda>\",\n",
      "                \"error\": \"Error while executing check function: ValueError(\\\"time data \\\"10-10-2021\\\" doesn't match format \\\"%Y-%m-%d\\\", at position 10. You might want to try:    - passing `format` if your strings have a consistent format;    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.\\\")Traceback (most recent call last):  File \\\"/home/sp/miniconda3/envs/PyValidationTalk/lib/python3.10/site-packages/pandera/backends/pandas/components.py\\\", line 235, in run_checks    self.run_check(  File \\\"/home/sp/miniconda3/envs/PyValidationTalk/lib/python3.10/site-packages/pandera/backends/pandas/base.py\\\", line 115, in run_check    check_result: CheckResult = check(check_obj, *args)  File \\\"/home/sp/miniconda3/envs/PyValidationTalk/lib/python3.10/site-packages/pandera/api/checks.py\\\", line 234, in __call__    return backend(check_obj, column)  File \\\"/home/sp/miniconda3/envs/PyValidationTalk/lib/python3.10/site-packages/pandera/backends/pandas/checks.py\\\", line 348, in __call__    check_output = self.apply(check_obj)  File \\\"/home/sp/miniconda3/envs/PyValidationTalk/lib/python3.10/site-packages/pandera/backends/pandas/checks.py\\\", line 147, in apply    return apply_fn(check_obj)  File \\\"/home/sp/miniconda3/envs/PyValidationTalk/lib/python3.10/site-packages/pandera/backends/pandas/checks.py\\\", line 155, in apply_field    return self.check_fn(check_obj)  File \\\"/tmp/ipykernel_504376/3948721483.py\\\", line 12, in <lambda>    coerce_datetime = pa.Check(lambda s: pd.to_datetime(s) is not None)  File \\\"/home/sp/miniconda3/envs/PyValidationTalk/lib/python3.10/site-packages/pandas/core/tools/datetimes.py\\\", line 1067, in to_datetime    values = convert_listlike(arg._values, format)  File \\\"/home/sp/miniconda3/envs/PyValidationTalk/lib/python3.10/site-packages/pandas/core/tools/datetimes.py\\\", line 433, in _convert_listlike_datetimes    return _array_strptime_with_fallback(arg, name, utc, format, exact, errors)  File \\\"/home/sp/miniconda3/envs/PyValidationTalk/lib/python3.10/site-packages/pandas/core/tools/datetimes.py\\\", line 467, in _array_strptime_with_fallback    result, tz_out = array_strptime(arg, fmt, exact=exact, errors=errors, utc=utc)  File \\\"strptime.pyx\\\", line 501, in pandas._libs.tslibs.strptime.array_strptime  File \\\"strptime.pyx\\\", line 451, in pandas._libs.tslibs.strptime.array_strptime  File \\\"strptime.pyx\\\", line 583, in pandas._libs.tslibs.strptime._parse_with_formatValueError: time data \\\"10-10-2021\\\" doesn't match format \\\"%Y-%m-%d\\\", at position 10. You might want to try:    - passing `format` if your strings have a consistent format;    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"SCHEMA\": {\n",
      "        \"WRONG_DATATYPE\": [\n",
      "            {\n",
      "                \"schema\": null,\n",
      "                \"column\": \"int between 0 and 1e6\",\n",
      "                \"check\": \"dtype('int64')\",\n",
      "                \"error\": \"expected series 'int between 0 and 1e6' to have type int64, got float64\"\n",
      "            },\n",
      "            {\n",
      "                \"schema\": null,\n",
      "                \"column\": \"no nulls allowed\",\n",
      "                \"check\": \"dtype('bool')\",\n",
      "                \"error\": \"expected series 'no nulls allowed' to have type bool, got object\"\n",
      "            }\n",
      "        ],\n",
      "        \"SERIES_CONTAINS_NULLS\": [\n",
      "            {\n",
      "                \"schema\": null,\n",
      "                \"column\": \"no nulls allowed\",\n",
      "                \"check\": \"not_nullable\",\n",
      "                \"error\": \"non-nullable series 'no nulls allowed' contains null values:10    NaNName: no nulls allowed, dtype: object\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sp/miniconda3/envs/PyValidationTalk/lib/python3.10/site-packages/pandera/backends/pandas/error_formatters.py:268: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  concat_fn(check_failure_cases)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spicy_pandera = my_pandera_schema(spicy_data, lazy=True)\n",
    "    print(\"If we got here my talk is going poorly ðŸ«¢\")\n",
    "except SchemaErrors as e:\n",
    "    print(f\"The bad dataset failed with errors:\\n{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy Pola.rs ðŸ»â€â„ï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Polars does what Pandas does... but has the benefit of learning from the mistakes Pandas had to discover.\n",
    "- Polars implements `lazy evaluation` through its Lazy API\n",
    "  - lazy == doesn't do stuff until its ready and has a plan\n",
    "- This comes with some schema functionality!\n",
    "- But it only covers typing and not validation ðŸ¥²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task failed succesfully with error: could not parse `934944.0` as dtype `i32` at column 'int between 0 and 1e6' (column number 3)\n",
      "\n",
      "The current offset in the file is 7 bytes.\n",
      "\n",
      "You might want to try:\n",
      "- increasing `infer_schema_length` (e.g. `infer_schema_length=10000`),\n",
      "- specifying correct dtype with the `schema_overrides` argument\n",
      "- setting `ignore_errors` to `True`,\n",
      "- adding `934944.0` to the `null_values` list.\n",
      "\n",
      "Original error: ```remaining bytes non-empty```\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from polars.exceptions import ComputeError\n",
    "\n",
    "\n",
    "def pl_validator(file_path: str) -> pl.DataFrame:\n",
    "    \"\"\"lazy evaluation of a polars DataFrame with a defined schema\"\"\"\n",
    "    schema = {\n",
    "        \"index\": pl.Int64,\n",
    "        \"text\": pl.Utf8,\n",
    "        \"int between 0 and 1e6\": pl.Int32,\n",
    "        \"float between -1 and 1\": pl.Float32,\n",
    "        \"no nulls allowed\": pl.Boolean,\n",
    "        \"iso 8601 compliant date\": pl.Date,\n",
    "    }\n",
    "\n",
    "    return pl.scan_csv(file_path, schema=schema).collect()\n",
    "\n",
    "\n",
    "# Usage with some nice data...\n",
    "good_pl = pl_validator(config.good_data)\n",
    "# and with the spicy data ðŸŒ¶ï¸\n",
    "try:\n",
    "    spicy_pl = pl_validator(config.spicy_data)\n",
    "    print(\"Oh god, we shouldn't get here... ðŸ˜±\")\n",
    "except ComputeError as e:\n",
    "    print(f\"Task failed succesfully with error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can always roll our own data-checks, with the caveat its not as easy to follow as `Pandera` or as descriptive.\n",
    "- If anyone knows a neat way to do these checks in Pola.rs feel free to interrupt me! ðŸ˜Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task failed succesfully with error: Validation failed, missing rows: 1\n"
     ]
    }
   ],
   "source": [
    "def pl_checker(file_path: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Roll our own checks...\n",
    "    Since filter drops anything that doesn't match the condition we check against\n",
    "    initial row count and fail if the count is less than the original\n",
    "    \"\"\"\n",
    "    # add our checks...\n",
    "    df = pl.scan_csv(file_path)\n",
    "    valid_df = df.filter(\n",
    "        (pl.col(\"int between 0 and 1e6\").is_between(0, 1e6))\n",
    "        & (pl.col(\"float between -1 and 1\").is_between(-1, 1))\n",
    "        & (pl.col(\"text\").str.starts_with(\"row\"))\n",
    "        & (pl.col(\"no nulls allowed\").is_not_null())\n",
    "    ).collect()\n",
    "    df = df.collect()\n",
    "    if len(valid_df) < len(df):\n",
    "        # this is SO BAD. My ignorance of polars is showing here...\n",
    "        # surely there would be a neat way we can do this with polars and have it return descriptive errors\n",
    "        # but then again, maybe you should be using Pandera or some database tooling to do this hey?\n",
    "        raise ValueError(f\"Validation failed, missing rows: {len(df) - len(valid_df)}\")\n",
    "    return valid_df\n",
    "\n",
    "\n",
    "good_pl = pl_checker(config.good_data)\n",
    "try:\n",
    "    spicy_pl = pl_checker(config.spicy_data)\n",
    "    print(\"Please dont print this ðŸ˜…\")\n",
    "except ValueError as e:\n",
    "    print(f\"Task failed succesfully with error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pydantic\n",
    "\n",
    "- Advantages: âœ…\n",
    "  - You may want to use this anyway for validating your *not table* data\n",
    "  - It has a rich feature set for validation\n",
    "- Disadvantages: ðŸš«\n",
    "  - Not beginner friendly!\n",
    "  - Big API change in v2 which means DeepSeek and Stackoverflow often suggest the wrong thing.\n",
    "  - Not designed for tabular data\n",
    "- We're going to use version >= v2.10.0 here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: `Field(ge=0, le=1e6)` will check that `0 >= x <= 1e6`. Elegant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The good dataset passed ðŸ˜Ž\n",
      "The bad dataset failed with error: 4 validation errors for MyModelValidator\n",
      "text\n",
      "  Value error, must start with \"row\" [type=value_error, input_value='0', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/value_error\n",
      "int_between_0_and_1e6\n",
      "  Input should be greater than or equal to 0 [type=greater_than_equal, input_value=-1.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/greater_than_equal\n",
      "float_between_minus_1_and_1\n",
      "  Input should be less than or equal to 1 [type=less_than_equal, input_value=6.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/less_than_equal\n",
      "no_nulls_allowed\n",
      "  Input should be a valid boolean [type=bool_type, input_value=nan, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/bool_type\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field, field_validator, ValidationError\n",
    "\n",
    "\n",
    "class MyModelValidator(BaseModel):\n",
    "    text: str\n",
    "    int_between_0_and_1e6: int = Field(ge=0, le=1e6)\n",
    "    float_between_minus_1_and_1: float = Field(ge=-1, le=1)\n",
    "    no_nulls_allowed: bool\n",
    "\n",
    "    @field_validator(\"text\")\n",
    "    def text_must_start_with_row(cls, v):\n",
    "        if not v.startswith(\"row\"):\n",
    "            raise ValueError('must start with \"row\"')\n",
    "        return v\n",
    "\n",
    "\n",
    "def munge_colnames_for_pydantic(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    pydantic colnames can't have spaces or hyphens, so we replace them here\n",
    "    \"\"\"\n",
    "    df.copy()\n",
    "    df.columns = [c.replace(\" \", \"_\").replace(\"-\", \"minus_\") for c in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Validate good data\n",
    "we have to iterate through each row individually... inefficient!\n",
    "there's probably a more elegant setup here where you treat every field as a list\n",
    "and validate the types INSIDE each list\n",
    "\"\"\"\n",
    "munged_good_data = munge_colnames_for_pydantic(good_data)\n",
    "for row in munged_good_data.to_dict(orient=\"records\"):\n",
    "    MyModelValidator(**row)\n",
    "print(\"The good dataset passed ðŸ˜Ž\")\n",
    "\n",
    "# Validate spicy data\n",
    "munged_spicy_data = munge_colnames_for_pydantic(spicy_data)\n",
    "try:\n",
    "    for row in munged_spicy_data.to_dict(orient=\"records\"):\n",
    "        MyModelValidator(**row)\n",
    "    print(\"Everyone will find out I just used chatGPT ðŸ˜­\")\n",
    "except ValidationError as e:\n",
    "    print(f\"The bad dataset failed with error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Protip Pandera has a `DataFrameModel` that plays nicely with Pydantic.\n",
    "- This is probably what you want if you want to do DataFrame validation alongside Pydantic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The good dataset passed ðŸ˜Ž\n",
      "The bad dataset failed with errors:\n",
      "Column 'text' failed element-wise validator number 0: str_startswith('row') failure cases: 0\n"
     ]
    }
   ],
   "source": [
    "from pandera.typing import Series\n",
    "\n",
    "class MyPanderaModel(pa.DataFrameModel):\n",
    "    \"\"\"Pandera DataFrameModel for validation\"\"\"\n",
    "    text: Series[str] = pa.Field(str_startswith=\"row\")\n",
    "    int_between_0_and_1e6: Series[int] = pa.Field(ge=0, le=1e6)\n",
    "    float_between_minus_1_and_1: Series[float] = pa.Field(ge=-1, le=1)\n",
    "    no_nulls_allowed: Series[bool] = pa.Field(nullable=False)\n",
    "    # I can't seem to get custom checks working in pa.Field...\n",
    "    # so no validation check for the iso date here.\n",
    "    iso_8601_compliant_date: Series[str] = pa.Field()\n",
    "\n",
    "good_pandera = MyPanderaModel(good_data)\n",
    "print(\"The good dataset passed ðŸ˜Ž\")\n",
    "try:\n",
    "    # we can still do a lazy evaluation with Pandera `DataFrameModel`\n",
    "    spicy_pandera = MyPanderaModel(spicy_data, lazy=False)\n",
    "    print(\"Can't print this!\")\n",
    "except SchemaError as e:\n",
    "    print(f\"The bad dataset failed with errors:\\n{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Demo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyValidationTalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
